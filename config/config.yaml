server:
  port: "${PORT:-8080}"
  master_key: "${GOMODEL_MASTER_KEY:-}"

# Cache configuration for model storage
# type: "local" (default) - file-based cache for single instance
# type: "redis" - Redis cache for multiple instances behind load balancer
cache:
  type: "${CACHE_TYPE:-local}"
  redis:
    url: "${REDIS_URL:-redis://localhost:6379}"
    key: "${REDIS_KEY:-gomodel:models}"
    ttl: 86400 # 24 hours in seconds

providers:
  openai-primary:
    type: "openai"
    api_key: "${OPENAI_API_KEY}"

  anthropic-primary:
    type: "anthropic"
    api_key: "${ANTHROPIC_API_KEY}"

  gemini-primary:
    type: "gemini"
    api_key: "${GEMINI_API_KEY}"

  # Example: Groq (OpenAI-compatible)
  # groq:
  #   type: "openai"
  #   base_url: "https://api.groq.com/openai/v1"
  #   api_key: "${GROQ_API_KEY}"

  # Example: Azure OpenAI
  # azure-openai:
  #   type: "openai"
  #   base_url: "https://your-resource.openai.azure.com/openai/deployments/your-deployment"
  #   api_key: "${AZURE_OPENAI_API_KEY}"

  # Example: DeepSeek (OpenAI-compatible)
  # deepseek:
  #   type: "openai"
  #   base_url: "https://api.deepseek.com/v1"
  #   api_key: "${DEEPSEEK_API_KEY}"
